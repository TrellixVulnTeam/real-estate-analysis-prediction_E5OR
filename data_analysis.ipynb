{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1ZSicm72MWB"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjSS4LV72Ldg"
      },
      "source": [
        "%pyspark\n",
        "basic_folder = \"hdfs://master:9000/user/hadoop/dataset/\"\n",
        "\n",
        "provinces = [\"강원도\",\"경기도\",\"경상남도\",\"경상북도\",\"광주광역시\",\n",
        "             \"대구광역시\",\"대전광역시\",\"부산광역시\",\"서울특별시\",\n",
        "             \"세종특별자치시\",\"울산광역시\",\"인천광역시\",\"전라남도\",\n",
        "             \"전라북도\",\"제주특별자치도\",\"충청남도\",\"충청북도\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TFgoYFk17AK"
      },
      "source": [
        "%pyspark\n",
        "for p in provinces:\n",
        "    files = basic_folder + p + '/' + p + '_*' \n",
        "    print(files)\n",
        "    df = spark.read.csv(files, header=True, inferSchema=True)\n",
        "    df.createOrReplaceTempView('`' + p + '`')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xU4bxX2Q2DF6"
      },
      "source": [
        "# 전국 단위 테이블 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaOZho9X1-7x"
      },
      "source": [
        "%pyspark\n",
        "path = \"hdfs://master:9000/user/hadoop/dataset/*/*\"\n",
        "df = spark.read.csv(path, header=True, inferSchema=True)\n",
        "df.createOrReplaceTempView('`전국`')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDjkemfRf59I"
      },
      "source": [
        "# 전국 집값 추이"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5aoOjrKf6EM"
      },
      "source": [
        "%sql\n",
        "SELECT substr(`계약년월`, 1, 4) as `연도`,\n",
        "       cast(avg(regexp_replace(`거래금액(만원)`, ',', '')) as decimal) as `가격`\n",
        "FROM `전국`\n",
        "GROUP BY `연도`"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_0AMu92f6KI"
      },
      "source": [
        "# 전국 지역별 집값 추이"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJnvJ0tWf6QU"
      },
      "source": [
        "%sql\n",
        "SELECT substr(`시군구`, 1, instr(`시군구`, ' ')) as `지역`,\n",
        "       substr(`계약년월`, 1, 4) as `연도`,\n",
        "       cast(avg(regexp_replace(`거래금액(만원)`, ',', '')) as decimal) as `가격`\n",
        "FROM `전국`\n",
        "GROUP BY `지역`, `연도`\n",
        "ORDER BY `연도`"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ig-R6Ccdf6V-"
      },
      "source": [
        "# 서울시 구별 집값 추이"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXT6piW2f6ct"
      },
      "source": [
        "%sql\n",
        "SELECT substr(substring_index(`시군구`, ' ', 2), instr(`시군구`, ' ') + 1, 3) as `지역`,\n",
        "       substr(`계약년월`, 1, 4) as `연도`,\n",
        "       cast(avg(regexp_replace(`거래금액(만원)`, ',', '')) as decimal) as `가격`\n",
        "FROM `서울특별시`\n",
        "GROUP BY `지역`, `연도`\n",
        "ORDER BY `연도`"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKWk7J3v2WOI"
      },
      "source": [
        "# 1. 특정시에 대한 집값 추이"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0SmbZIF2WVV"
      },
      "source": [
        "%pyspark\n",
        "df_province = sqlContext.sql(\"\"\"\n",
        "select substr(`계약년월`, 1, 4) as y, \n",
        "       cast(avg(regexp_replace(`거래금액(만원)`, ',', '')) as decimal) as price\n",
        "from `\"\"\" + z.get(\"province\") + \"\"\"`\n",
        "group by y\n",
        "order by y\n",
        "\"\"\")\n",
        "z.show(df_province)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hew4xflF2Wb3"
      },
      "source": [
        "# 2. 상세 행정동에 대한 집값 추이"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_w6kB2i2Wht"
      },
      "source": [
        "%pyspark\n",
        "df_detailAddress = sqlContext.sql(\"\"\"\n",
        "select substr(`계약년월`, 1, 4) as year, \n",
        "       cast(avg(regexp_replace(`거래금액(만원)`, ',', '')) as decimal) as price\n",
        "from `\"\"\" + z.get(\"province\") + \"\"\"`\n",
        "where `시군구` like '%\"\"\" + z.get(\"detailAddress\") + \"\"\"%'\n",
        "group by year\n",
        "order by year\n",
        "\"\"\")\n",
        "z.show(df_detailAddress)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0wKte5t2Wnu"
      },
      "source": [
        "# 3. 특정 아파트 집값 추이"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_wPrplf2WuN"
      },
      "source": [
        "%pyspark\n",
        "df_apart = sqlContext.sql(\"\"\"\n",
        "select substr(`계약년월`, 1, 4) as y, \n",
        "       cast(avg(regexp_replace(`거래금액(만원)`, ',', '')) as decimal) as price\n",
        "from `\"\"\" + z.get(\"province\") + \"\"\"`\n",
        "where `단지명` like '\"\"\" + z.get(\"apart\") + \"\"\"%'\n",
        "group by y\n",
        "order by y\n",
        "\"\"\")\n",
        "z.show(df_apart)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7Crtew-2W0G"
      },
      "source": [
        "# 4. 검색한 아파트의 층별 집값"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PG9kAA7Z2W5V"
      },
      "source": [
        "%pyspark\n",
        "df_floor = sqlContext.sql(\"\"\"\n",
        "select `층` as floor, \n",
        "       cast(avg(regexp_replace(`거래금액(만원)`, ',', '')) as decimal) as price\n",
        "from `\"\"\" + z.get(\"province\") + \"\"\"`\n",
        "where (`단지명` like '\"\"\" + z.get(\"apart\") + \"\"\"%')\n",
        "group by floor\n",
        "order by floor\n",
        "\"\"\")\n",
        "z.show(df_floor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLzX4B3D2W-X"
      },
      "source": [
        "# 5. 검색한 행정동에서 최신집값이 내가 원하는 집값 사이에 있는 곳"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cByAqNyX2XDB"
      },
      "source": [
        "%pyspark\n",
        "df_recent = sqlContext.sql(\"\"\"\n",
        "select `단지명` as apt, max(`계약년월`) as `계약년월`, \n",
        "       max(cast(regexp_replace(`거래금액(만원)`, ',', '') as decimal)) as price, \n",
        "       round(`전용면적(㎡)`, 2), max(`건축년도`)\n",
        "from `\"\"\" + z.get(\"province\") + \"\"\"`\n",
        "where cast(regexp_replace(`거래금액(만원)`, ',', '') as decimal) \n",
        "      between \"\"\" + z.get(\"price_up\") + \"\"\" and \"\"\" + z.get(\"price_down\") + \"\"\" \n",
        "      and (`시군구` like '%\"\"\" + z.get(\"detailAddress\") + \"\"\"%') and `계약년월` like '2021%'\n",
        "group by apt, `전용면적(㎡)`\n",
        "order by `계약년월` desc\n",
        "\"\"\")\n",
        "z.show(df_recent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lkwz3Sm23hnh"
      },
      "source": [
        "# 6. 집값 예측모델"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12dGuab03ht8"
      },
      "source": [
        "%pyspark\n",
        "# 아파트 최근 데이터 (train dataset)\n",
        "df_apart_recent = sqlContext.sql(\"\"\"\n",
        "select `계약년월` as year, \n",
        "       cast(avg(regexp_replace(`거래금액(만원)`, ',', '')) as decimal) as price\n",
        "from `\"\"\" + z.get(\"province\") + \"\"\"`\n",
        "where `단지명` like '\"\"\" + z.get(\"apart\") + \"\"\"%'\n",
        "group by year\n",
        "order by year desc limit 6\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fV67O58R3jh0"
      },
      "source": [
        "%pyspark\n",
        "df_apart.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IC9sU1s3jlG"
      },
      "source": [
        "%pyspark\n",
        "# 아파트 검색 -> 예측값 반환\n",
        "# get next year for pred_dataset\n",
        "df_apart_recent.registerTempTable(\"temp\")\n",
        "df_pred_y = sqlContext.sql(\"\"\"\n",
        "select max(y) + 1 as y\n",
        "from temp\n",
        "\"\"\")\n",
        "\n",
        "# trainset\n",
        "from pyspark.sql.types import DateType\n",
        "df_apart_recent = df_apart_recent.withColumn(\"y\", df_apart_recent['y'].cast('int')) \n",
        "\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "feature_columns = df_apart_recent.columns[:-1]\n",
        "assembler = VectorAssembler(inputCols=feature_columns,outputCol=\"year\")\n",
        "dataset = assembler.transform(df_apart_recent)\n",
        "train, test = dataset.randomSplit([1.0, 0.0]) # train 100% of dataset\n",
        "\n",
        "# model and train\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "algo = LinearRegression(featuresCol=\"year\", labelCol=\"price\")\n",
        "model = algo.fit(train)\n",
        "\n",
        "# prediction\n",
        "testset = assembler.transform(df_pred_y)\n",
        "predictions = model.transform(testset)\n",
        "\n",
        "# get predicted val\n",
        "import pyspark.sql.functions as f\n",
        "pred_val = predictions.select(f.collect_list('prediction')).first()[0][0]\n",
        "print(round(pred_val, 2), \" (단위 : 만원)\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}